---
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Homework 4 - Classification

Research Question: Our research question for a classification task will utilize the “qb_logs” dataframe. Using various measures of starting quarterback performance in a specific game (Passes Completed, Passes Attempted, Completion Percentage, Passing Yards, Passing Yards Per Attempt, TD Passes, INTs, Sacks, Sacked Yards Lost, Passer Rating, Rushing Attempts, Rushing Yards, Yards Per Carry, Rushing TDs, Fumbles, Fumbles Lost), we want to predict the outcome of the game (W or L) for the quarterback’s team. 

```{r, eval = TRUE, include = FALSE}
# library statements 
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels) 
library(tidyverse)
library(probably)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions

set.seed(123)

basic_stats <- read_csv("Basic_Stats.csv")
qb_stats <- read_csv("Career_Stats_Passing.csv")
punt_return <- read_csv("Career_Stats_Punt_Return.csv")
punting <- read_csv("Career_Stats_Punting.csv")
receiving <- read_csv("Career_Stats_Receiving.csv")
kickers <- read_csv("Game_Logs_Kickers.csv")
oline_logs <- read_csv("Game_Logs_Offensive_Line.csv")
punters_logs <- read_csv("Game_Logs_Punters.csv")
qb_logs <- read_csv("Game_Logs_Quarterback.csv",na=c("","--","NA"))
rb_logs <- read_csv("Game_Logs_Runningback.csv")
wrandte_logs <- read_csv("Game_Logs_Wide_Receiver_and_Tight_End.csv")
```

```{r}
# data cleaning
qb_stats <- qb_stats %>%
    select(-Position)
    
qb_stats <- qb_stats %>%
    rename("Rating" = "Passer Rating")

basic_stats <- basic_stats %>%
    rename("Team" = "Current Team")

basic_stats <- basic_stats %>%
    rename("hsloc" = "High School Location")

basic_stats <- basic_stats %>%
    rename("Weight" = "Weight (lbs)")

basic_stats <- basic_stats %>%
    rename("Height" = "Height (inches)")

basic_stats <- basic_stats %>%
    filter(Position == "QB")

qb_comb <- merge(qb_stats, basic_stats, by="Name")


qb_comb$`Passes Attempted`[which(qb_comb$`Passes Attempted` == '--')] = 0
qb_comb <- qb_comb %>%
  mutate(`Passes Attempted` = as.numeric(`Passes Attempted`)) %>% 
    mutate(`Games Played` = as.numeric(`Games Played`))

qb_comb <- qb_comb %>% select(-c(Number, `Years Played`))


qb_comb <- qb_comb %>% mutate(Experience = as.numeric(stringr::str_extract(Experience,'[0-9]+')))



#qb_comb$hsloc
#####################

qb_logs <- qb_logs %>% 
  filter(Season != "Preseason", Outcome != "T", `Games Started` == 1) %>% 
  select(-c(Position, Week, `Game Date`, Score, `Games Played`, `Games Started`, Name, `Player Id`, Year, Season, Opponent, `Home or Away`))

qb_logs$Outcome <- as.factor(qb_logs$Outcome) 

qb_logs <- qb_logs %>% 
  mutate(Outcome = relevel(Outcome, ref='L')) #set reference level to be L

qb_logs[is.na(qb_logs)] = 0  #replaces NA values with 0
```



## Logistic Regression Model

The first method we will explore to answer our classification question is a logistic regression model. 

```{r}
logistic_mod <- logistic_reg() %>%
    set_engine("glm") %>%
    set_mode('classification')

outcome_rec <- recipe(Outcome ~ ., data = qb_logs) %>%
  step_normalize(all_numeric_predictors())

logistic_mod_fit <- workflow() %>%
  add_model(logistic_mod) %>%
  add_recipe(outcome_rec) %>%
  fit(data = qb_logs)
```

Here we show the output (model coefficients) from our trained logistic regression model. Then, we use a dot-and-whisker plot to examine the odds ratios represented by the exponentiated coefficient each predictor variable. 

```{r}
logistic_mod_fit %>%
  tidy() # Model Coefficients from Trained Model

library(dotwhisker)
tidy(logistic_mod_fit) %>%  # Viz of Trained Model Odds Ratios
  mutate(conf.low = exp(estimate - 1.96*std.error),conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(estimate = exp(estimate)) %>% # this second
  dwplot(dot_args = list(size = 2, color = "black"),
         whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 1, color = "grey50", linetype = 2)) + 
  labs(x = 'Odds Ratio') + 
  theme_classic()
```

A table that outputs our soft predictions for each game (case) in our dataframe.

```{r}
# Soft Predictions: Predicted Probabilities
logistic_output <-  qb_logs %>%
  bind_cols(predict(logistic_mod_fit, new_data = qb_logs, type = 'prob')) 

logistic_output %>% head()
```

A box-and-whisker plot showing our predicted probability of winning vs. observed outcome (whether the QB's team won or lost the game). We can see from this plot that for QBs who lost the game (L), the median predicted probability of winning was around 0.25. For QBs who won the game (W), the median predicted probability of winning was around 0.75. 

```{r}
# Visualize predicted probabilities as a function of true outcome
logistic_output %>%
  ggplot(aes(x = Outcome, y = .pred_W)) +
  geom_boxplot() + 
  labs(y = 'Predicted Probability of Winning', x = 'Observed Outcome (L or W)') +
  theme_classic()
```

Next, we evaluate the accuracy of our logistic regression model using a confusion matrix. Our model has an overall accuracy of 77.14% (7119 correct predictions / 9229 total cases). The sensitivity (true positive rate, where a positive is a W) of our model is 78.05% (3961 / 4999). The specificity (true negative rate, where a negative is a L) of our model is 76.18% (3428 / 4500). In the overall context of our research question, there are no advantages to favoring the sensitivity over the specificity or vice versa. At least for our purposes, the consequences of a false positive (predicting a QB W when the observed outcome is an L) are no worse or better than the consequences of a false negative (predicting a QB L when the observed outcome is a W). 

```{r}
logistic_output %>%
  ggplot(aes(x = Outcome, y = .pred_L)) +
  geom_boxplot() + 
  geom_hline(yintercept = 0.5, color='red') + 
  labs(y = 'Predicted Probability of LOSING', x = 'Observed Outcome (L or W)') +
  theme_classic()

logistic_output <- logistic_output %>%
  mutate(.pred_class = make_two_class_pred(.pred_L, levels(Outcome), threshold = .5)) #77.1 accuracy

#head(logistic_output)

#logistic_output %>%
#  count(Outcome, .pred_class)

logistic_output %>%
  conf_mat(truth = Outcome, estimate = .pred_class)
```



## Random Forest Model

The second method we explore to answer our classification question is a random forest model.

```{r}
outcome_rf_rec <- recipe(Outcome ~ ., data = qb_logs) %>%
  step_normalize(all_numeric_predictors())

rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(ncol(x)))
           trees = 100, # Number of bags
           min_n = 5,
           probability = FALSE, # want hard predictions first
           importance = 'impurity') %>% 
  set_mode('classification') # change this for regression tree

rf_spec


outcome_rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(outcome_rf_rec)
```

Here we calculate the OOB prediction error of our random forest model. From this value, we can calculate that our model accuracy is 1 - 0.2274 = 0.7726 or 77.26%.

```{r}
set.seed(123)
outcome_rf_fit <- outcome_rf_wf %>%
  fit(data = qb_logs)

outcome_rf_fit # check out OOB prediction error (accuracy = 1 - OOB prediction error)
```

We also can calculate the sensitivity and specificity of our random forest model. Our model has a sensitivity of 77.89% and a specificity of 76.65%. As explained above, in the context of our data and research question, there is no advantage to prioritizing sensitivity over specificity or vice versa. 

```{r}
outcome_rf_OOB_output <- tibble(
  .pred_class = outcome_rf_fit %>% extract_fit_engine() %>% pluck('predictions'),
  Outcome = qb_logs %>% pull(Outcome))

bag_metrics <- metric_set(sens, yardstick::spec, accuracy)

outcome_rf_OOB_output %>% 
  bag_metrics(truth = Outcome, estimate = .pred_class)
```

```{r}
set.seed(123) #to get the same bootstrap samples, use same seed
outcome_rf_fit2 <- outcome_rf_wf %>%
  update_model(rf_spec %>% set_args(probability = TRUE)) %>%
  fit(data = qb_logs)

outcome_rf_fit2
```

The area under the ROC curve is approximately 0.851, indicating a strong performance
of our model using bootstrapped data. 

```{r}
outcome_rf_OOB_output2 <- bind_cols(
  outcome_rf_fit2 %>% extract_fit_engine() %>% pluck('predictions') %>% as_tibble(),
  qb_logs %>% select(Outcome))

outcome_rf_OOB_output2 %>% 
  roc_curve(Outcome, W, event_level = "second") %>% autoplot()

outcome_rf_OOB_output2 %>% 
  roc_auc(Outcome, W, event_level = "second") #Area under Curve
```

Finally, we have two different bar chart charts using our random forest model to give an indication of predictor variable importance. The first bar chart compares our predictors using their impurity (the total decrease in node impurities splitting on each predictor, averaged over all trees in the model). The second bar chart compares our predictors using their permutation importance (the difference between two accuracy values: a tree’s prediction accuracy in the OOB observations and the tree’s prediction accuracy computed after any association between the predictor and the outcome is broken by permuting the values of all individuals within that variable). We can see that Passer Rating is the most important variable according to both measures. However, after Passer Rating, variable importance varies between the two measures.

```{r}
library(vip) #install.packages('vip')

outcome_rf_fit %>% extract_fit_engine() %>% vip() #based on impurity

outcome_rf_wf %>% #based on permutation
  update_model(rf_spec %>% set_args(importance = "permutation")) %>%
  fit(data = qb_logs) %>% extract_fit_engine() %>% vip()
```

### Summarizing Conclusions

Overall, our random forest model performs well on the given data.
Without bootstrapping, it generates a sensitivity of 77.89% and specificity of
76.65%, showing a relatively strong performance in terms of true positive and
true negative rates. It returns an accuracy rate of 77.25%.
Bootstrapping the data also results in a strong performance, as illustrated by 
the area under the ROC, 0.85. 

Comparing our random forest model to the logistic regression model, we see that
there is no substantial difference in performance. Our logistic regression model
returns similar rates for sensitivity of 78.05% and specificity of 76.18%.
Its accuracy rate of 77.14% is also similar to that of our random forest model.

Both models perform similarly. While there are marginal differences in terms
of sensitivity, specificity, and accuracy, they aren't enough to definitively select one over the other.

### Classification - Conclusions

As previously mentioned, both random forest and logistic regression models 
perform relatively well on the data. However, we have a slight preference for the random 
forest model, given that it is slightly more accurate than the logistic model, 
by 0.1%. 

Let's take a look at both models' test confusion matrices. The logistic model
accurately predicts a loss 3428 / (3428 + 1038) = 76.76% of the time, while
the random forest model accurately predicts a loss 3505 / (3505 + 1104) = 
76.05% of the time. The logistic model is marginally more accurate in predicting 
a loss given a quarterback's performance, which is measured by the given
predictors. 

The logistic regression model accurately predicts a win 3691 / (3691 + 1072) = 
77.49% of the time, while the random forest model accurately predicts a win
78.46% of the time. The random forest model is marginally more accurate in 
predicting a win given a quarterback's performance, which is measured by 
the given predictors. 

The random forest model also shows consistent performance in terms of variable
importance for both impurity and permutation. Passer Rating, Passes Attempted, 
and Passing Yards per Attempt are consistently among the most important 
predictors of game outcome. While the other predictors vary in terms of
importance, the model is still relatively consistent, and this is one of its
strengths.

```{r}
logistic_output %>%
  conf_mat(truth = Outcome, estimate = .pred_class)

outcome_rf_OOB_output %>%
  conf_mat(truth = Outcome, estimate = .pred_class)

```